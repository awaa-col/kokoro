# ================================================================
# Kokoro-Mamba Pro PEFT 项目 - 最终配置文件 (v2.0)
# ================================================================
# 这是最终的、与 train.py 完全同步的配置文件。

# --- 1. 路径配置 (Paths) ---
paths:
  repo_id: "hexgrad/Kokoro-82M-v1.1-zh"
  # 【重要】这里现在指向你完整解压后的 AISHELL-3 train 文件夹
  aishell3_unzipped_train_dir: "./AISHELL-3/train"
  # 预处理后的数据存放位置
  processed_data: "./AISHELL-3-processed"
  # 模型权重输出目录
  output_dir: "./kokoro_finetuned_model_4070L"
  # (可选) 第二阶段微调时，加载第一阶段的 Mamba 权重
  load_mamba_from: null

# --- 2. 数据处理配置 (Data Processing) ---
data:
  # 使用 100% 的全量数据
  sampling_ratio: 1.0 # (此参数在当前模式下无效，但保留)
  target_sample_rate: 24000

# --- 3. 训练配置 (Training) ---
training:
  # 4070 Laptop 8G 显存的保守设置
  batch_size: 4
  epochs: 50
  learning_rate: 1e-4
  use_amp: true
  save_every_n_epochs: 5
  num_workers: 4

# --- 4. Mamba 模型配置 (Mamba Model) ---
mamba:
  d_state: 16
  d_conv: 4
  expand: 2
