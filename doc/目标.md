[当前任务进行中]

# 微调方案：Kokoro 模型现代化改造计划

## 1. 核心目标

[Priority: 最高] [Feasibility/Necessity: 核心] [Status: Not Implemented]

将 `kokoro` 模型中的核心时序处理模块从老旧的 `LSTM` 替换为现代化的 `Mamba` 架构，并进行参数高效微调（PEFT），以最小的计算成本显著提升模型的韵律建模能力和推理效率。

---

## 2. 实施计划

### 任务一：环境准备与依赖安装

-   **描述**: 为项目引入 `Mamba` 架构所需的依赖。没有工具，还想造火箭？
-   **实施细节**:
    1.  安装 `mamba-ssm` 包。这是官方实现，别自己瞎找。
        ```bash
        pip install mamba-ssm causal-conv1d
        ```
    2.  确认你的 `torch` 版本与 `mamba-ssm` 兼容。如果不兼容，自己解决，别来烦我。
-   **标签**:
    -   `[Priority]`: **最高**
    -   `[Implementation Difficulty]`: **低**
    -   `[Feasibility/Necessity]`: **必要**
    -   `[Status]`: `[Not Implemented]`

### 任务二：模型外科手术 (Model Surgery)

-   **描述**: 这是整个计划的核心。我们要像一个外科医生一样，精确地切除位于 `DurationEncoder` 中的 `LSTM` 肿瘤，并移植上 `Mamba` 这个强大的新器官。
-   **实施细节**:
    1.  **定位文件**: `LNN-1/kokoro/kokoro/modules.py`。
    2.  **找到目标**: `DurationEncoder` 类。看到里面那堆 `nn.LSTM` 了吗？对，就是它，丑陋又低效。
    3.  **执行替换**:
        -   在文件头部引入 Mamba: `from mamba_ssm import Mamba`。
        -   在 `DurationEncoder` 的 `__init__` 方法中，注释或删除所有 `self.lstms` 相关的代码。
        -   实例化 Mamba 模块。一个 Mamba 块就足以替代原来那一堆 LSTM 了。
            ```python
            # 原来的 LSTM 堆栈
            # self.lstms = nn.ModuleList()
            # for _ in range(nlayers):
            #     self.lstms.append(nn.LSTM(...))
            #     self.lstms.append(AdaLayerNorm(...))

            # 替换为 Mamba
            self.mamba = Mamba(
                d_model=d_model + sty_dim, # 输入维度需要匹配
                d_state=16,  # 状态维度，可以调整
                d_conv=4,    # 卷积核大小，可以调整
                expand=2,    # 扩展因子，可以调整
            )
            ```
        -   修改 `forward` 方法。`Mamba` 不需要 `pack_padded_sequence` 这种伺候 `LSTM` 的繁琐操作。直接将数据喂给它。
            ```python
            # ... 省略前面的数据准备代码 ...
            # x 的维度应为 [batch, sequence_length, d_model]
            s = style.expand(x.shape[0], x.shape[1], -1)
            x = torch.cat([x.transpose(1, 2), s], axis=-1) # 准备输入
            
            # 原来的 LSTM 循环，全部干掉
            # for block in self.lstms:
            #     ...
            
            # 直接调用 Mamba
            x = self.mamba(x)
            
            # ... 返回前确保维度正确 ...
            return x.transpose(-1, -2)
            ```
-   **标签**:
    -   `[Priority]`: **最高**
    -   `[Implementation Difficulty]`: **高** (需要你动点脑子，别指望复制粘贴就完事)
    -   `[Feasibility/Necessity]`: **核心**
    -   `[Alternative Plan]`: 使用 LNN。但 Mamba 的社区支持和现成实现更好，LNN 可能需要你自己手搓，难度更高。我推荐简单的路。
    -   `[Status]`: `[Not Implemented]`

### 任务三：参数高效微调 (PEFT) 设置

-   **描述**: 我们是来做微调的，不是来从头炼丹的。必须冻结绝大部分参数，只训练我们新加入的 `Mamba` 模块。
-   **实施细节**:
    1.  加载预训练的 `KModel`。
    2.  冻结所有参数：
        ```python
        for param in model.parameters():
            param.requires_grad = False
        ```
    3.  **精准解冻**：只解冻你新添加的 Mamba 模块的参数。
        ```python
        # 假设你的 Mamba 模块路径是 model.predictor.text_encoder.mamba
        for param in model.predictor.text_encoder.mamba.parameters():
            param.requires_grad = True
        ```
-   **标签**:
    -   `[Priority]`: **高**
    -   `[Implementation Difficulty]`: **中**
    -   `[Feasibility/Necessity]`: **必要** (否则你的显卡会哭)
    -   `[Status]`: `[Not Implemented]`

### 任务四：准备数据和训练

-   **描述**: 模型已经准备好，现在需要给它喂饭了。你需要准备一个高质量的 `(文本, 音频)` 对的数据集。
-   **实施细节**:
    1.  **数据集**: 准备你自己的数据集。格式就是纯文本和对应的 `.wav` 文件。
    2.  **编写训练脚本**:
        -   创建一个标准的 PyTorch 训练循环。
        -   **数据加载器**: 从数据集中加载 `(text, wav)` 对。
        -   **数据预处理**: 使用 `kokoro` 的 `KPipeline` 来处理文本（转为音素）和音频（提取 `ref_s` 参考风格）。
        -   **损失函数**: 为了简单起见，直接用 `L1 Loss` 计算生成的 mel-spectrogram 和真实的 mel-spectrogram 之间的差距。这就够了。
        -   **优化器**: `AdamW`，用一个较小的学习率，比如 `1e-4`。
        -   **训练循环**: 获取模型输出，计算损失，反向传播，更新参数。定期保存模型并生成样本试听。
-   **标签**:
    -   `[Priority]`: **高**
    -   `[Implementation Difficulty]`: **中**
    -   `[Feasibility/Necessity]`: **必要**
    -   `[Status]`: `[Not Implemented]`
